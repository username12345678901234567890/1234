{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMlWVUCkur11jtyMzphyq5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/username12345678901234567890/1234/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8zN2BsopaR4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "18abd494-811c-4ca4-8314-4c813f141ee7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "cannot assign to expression (ipython-input-3280588967.py, line 481)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3280588967.py\"\u001b[0;36m, line \u001b[0;32m481\u001b[0m\n\u001b[0;31m    self.cluster_centers[k] = (1-alpha)*c + alpha=z\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression\n"
          ]
        }
      ],
      "source": [
        "# %% [single-cell] Self-Organizing MoE v3.1 â€” E=16384 start, Lower S-threshold, Single-reporter, Growth+Prune, Device-safe\n",
        "import os, sys, json, time, glob, math, random, subprocess\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "# I/O\n",
        "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"./samples\", exist_ok=True)\n",
        "\n",
        "# ---- LPIPS (optional) ----\n",
        "_HAS_LPIPS=False\n",
        "try:\n",
        "    import lpips  # noqa\n",
        "    _HAS_LPIPS=True\n",
        "except Exception:\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"lpips==0.1.4\"])\n",
        "        import lpips\n",
        "        _HAS_LPIPS=True\n",
        "    except Exception:\n",
        "        _HAS_LPIPS=False\n",
        "\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[Device] {device}\")\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_root: str=\"/images\"\n",
        "    image_size: int=128\n",
        "    epochs: int=20\n",
        "    batch_size: int=192\n",
        "\n",
        "    # Colab-safe\n",
        "    num_workers: int=0\n",
        "    pin_memory: bool=True\n",
        "\n",
        "    # Optimizer (SGD for lighter state)\n",
        "    lr: float=2e-3\n",
        "    momentum: float=0.9\n",
        "    weight_decay: float=1e-4\n",
        "    amp: bool=True\n",
        "\n",
        "    # Latent/model\n",
        "    z_dim: int=512\n",
        "    hidden: int=512\n",
        "\n",
        "    # Experts / routing\n",
        "    init_experts_desired: int=16384\n",
        "    top_k: int=4\n",
        "    epsilon_start: float=0.20\n",
        "    epsilon_end: float=0.05\n",
        "    tau_high: float=1.8\n",
        "    tau_low: float=0.7\n",
        "\n",
        "    # Capacity\n",
        "    capacity_alpha: float=1.2\n",
        "\n",
        "    # Entropy/repulsion\n",
        "    gate_entropy_lambda_warm: float=0.05\n",
        "    gate_entropy_lambda_cool: float=0.01\n",
        "    repulsion_lambda: float=1e-3\n",
        "    repulsion_subset: int=256\n",
        "    repulsion_sigma: float=0.15\n",
        "\n",
        "    # Per-expert virtual neurons (memory-light)\n",
        "    lora_rank: int=2\n",
        "    adapters_per_expert: int=8\n",
        "\n",
        "    # Pattern reporting (lower threshold; single-reporter)\n",
        "    report_theta: float=1.2\n",
        "    report_K: int=4\n",
        "    report_M: int=2\n",
        "    review_Kp: int=6\n",
        "    review_Mp: int=2\n",
        "    dup_cos_thr: float=0.90\n",
        "\n",
        "    # Regulation & credit\n",
        "    big_share_thr: float=0.05\n",
        "    big_topN: int=5\n",
        "    share_tax_thr: float=0.10\n",
        "    share_tax_lambda: float=0.2\n",
        "    neg_interest: float=0.03\n",
        "    credit_decay: float=0.98\n",
        "    credit_false_penalty: float=1.0\n",
        "    credit_true_reward: float=2.0\n",
        "    credit_contrib_scale: float=5.0\n",
        "    credit_diversity_scale: float=1.0\n",
        "    bankrupcy_level: float=10.0\n",
        "    bankrupcy_streak: int=3\n",
        "\n",
        "    # Recruitment\n",
        "    team_base_Q: int=256\n",
        "    team_alpha_S: float=0.5\n",
        "    recruit_donor_bias: float=-0.20\n",
        "    recruit_new_bias: float=+0.30\n",
        "\n",
        "    # Pruning\n",
        "    enable_prune: bool=True\n",
        "    prune_min_keep: int=1024\n",
        "    prune_usage_eps: float=1e-3\n",
        "    prune_credit_level: float=10.0\n",
        "    prune_max_per_epoch: int=2048\n",
        "\n",
        "    # Saving\n",
        "    out_ckpt_dir: str=\"./checkpoints\"\n",
        "    out_samples_dir: str=\"./samples\"\n",
        "    sample_rows: int=4\n",
        "    seed: int=42\n",
        "\n",
        "cfg = Config()\n",
        "random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
        "\n",
        "# ---------------- Dataset ----------------\n",
        "IMG_EXTS=(\".png\",\".jpg\",\".jpeg\",\".bmp\",\".webp\")\n",
        "\n",
        "class PlainImageDataset(Dataset):\n",
        "    def __init__(self, root, size=128, train=True):\n",
        "        self.paths=[p for p in glob.glob(os.path.join(root,\"**\",\"*\"), recursive=True)\n",
        "                    if os.path.splitext(p)[1].lower() in IMG_EXTS]\n",
        "        if not self.paths:\n",
        "            raise RuntimeError(f\"No images found under {root}.\")\n",
        "        aug=[transforms.Resize(size), transforms.CenterCrop(size)]\n",
        "        if train:\n",
        "            aug.insert(1, transforms.RandomHorizontalFlip(0.5))\n",
        "            aug.insert(2, transforms.ColorJitter(0.1,0.1,0.05,0.02))\n",
        "        self.tf=transforms.Compose(aug+[\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5]*3,[0.5]*3)\n",
        "        ])\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, i):\n",
        "        return self.tf(Image.open(self.paths[i]).convert(\"RGB\"))\n",
        "\n",
        "full_ds=PlainImageDataset(cfg.data_root, size=cfg.image_size, train=True)\n",
        "val_ratio=0.02 if len(full_ds)>2000 else 0.1\n",
        "val_len=max(16, int(len(full_ds)*val_ratio))\n",
        "train_len=len(full_ds)-val_len\n",
        "train_ds, val_ds = random_split(full_ds,[train_len,val_len], generator=torch.Generator().manual_seed(cfg.seed))\n",
        "\n",
        "train_loader=DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "                        num_workers=cfg.num_workers, pin_memory=cfg.pin_memory, drop_last=True)\n",
        "val_loader=DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,\n",
        "                      num_workers=cfg.num_workers, pin_memory=cfg.pin_memory, drop_last=False)\n",
        "print(f\"[Data] train={len(train_ds)} val={len(val_ds)} batch={cfg.batch_size}\")\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "def conv_block(in_ch,out_ch,k=3,s=1,p=1):\n",
        "    return nn.Sequential(nn.Conv2d(in_ch,out_ch,k,s,p), nn.GroupNorm(8,out_ch), nn.SiLU())\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super().__init__()\n",
        "        ch=64\n",
        "        self.net=nn.Sequential(\n",
        "            conv_block(3,ch), conv_block(ch,ch),\n",
        "            conv_block(ch,ch*2,s=2), conv_block(ch*2,ch*2),\n",
        "            conv_block(ch*2,ch*4,s=2), conv_block(ch*4,ch*4),\n",
        "            conv_block(ch*4,ch*8,s=2), conv_block(ch*8,ch*8),\n",
        "            conv_block(ch*8,ch*8,s=2), conv_block(ch*8,ch*8),\n",
        "        )\n",
        "        self.proj=nn.Linear(ch*8*8*8, z_dim)\n",
        "    def forward(self,x):\n",
        "        h=self.net(x).view(x.size(0),-1)\n",
        "        return self.proj(h)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,z_dim):\n",
        "        super().__init__()\n",
        "        ch=64\n",
        "        self.fc=nn.Linear(z_dim, ch*8*8*8)\n",
        "        self.up=nn.Sequential(\n",
        "            conv_block(ch*8,ch*8),\n",
        "            nn.Upsample(scale_factor=2,mode='nearest'),\n",
        "            conv_block(ch*8,ch*8),\n",
        "            nn.Upsample(scale_factor=2,mode='nearest'),\n",
        "            conv_block(ch*8,ch*4),\n",
        "            nn.Upsample(scale_factor=2,mode='nearest'),\n",
        "            conv_block(ch*4,ch*2),\n",
        "            nn.Upsample(scale_factor=2,mode='nearest'),\n",
        "            conv_block(ch*2,ch),\n",
        "            nn.Conv2d(ch,3,3,1,1), nn.Tanh()\n",
        "        )\n",
        "    def forward(self,z):\n",
        "        h=self.fc(z).view(z.size(0),512,8,8)\n",
        "        return self.up(h)\n",
        "\n",
        "class ExpertLoRA(nn.Module):\n",
        "    def __init__(self, z_dim, hidden, W, rank):\n",
        "        super().__init__()\n",
        "        self.z_dim=z_dim; self.hidden=hidden; self.W=W; self.r=rank\n",
        "        self.a1=nn.Parameter(torch.randn(W, rank, z_dim)*0.02)   # [W,r,D]\n",
        "        self.b1=nn.Parameter(torch.randn(W, hidden, rank)*0.02)  # [W,H,r]\n",
        "        self.a2=nn.Parameter(torch.randn(W, rank, hidden)*0.02)  # [W,r,H]\n",
        "        self.b2=nn.Parameter(torch.randn(W, z_dim, rank)*0.02)   # [W,D,r]\n",
        "        self.scale=1.0\n",
        "    def forward_batch(self, z, a_idx, h_pre, out_pre):\n",
        "        dt = z.dtype\n",
        "        A1 = self.a1[a_idx].to(dt)\n",
        "        B1 = self.b1[a_idx].to(dt)\n",
        "        t1 = torch.einsum('nd,nrd->nr', z, A1)\n",
        "        d1 = torch.einsum('nr,nhr->nh', t1, B1) * self.scale\n",
        "        h  = F.silu(h_pre + d1).to(dt)\n",
        "        A2 = self.a2[a_idx].to(dt)\n",
        "        B2 = self.b2[a_idx].to(dt)\n",
        "        t2 = torch.einsum('nh,nrh->nr', h, A2)\n",
        "        d2 = torch.einsum('nr,ndr->nd', t2, B2) * self.scale\n",
        "        return (out_pre + d2).to(dt)\n",
        "\n",
        "class MoELatent(nn.Module):\n",
        "    def __init__(self, z_dim, hidden, init_experts, adapters_per_expert, rank):\n",
        "        super().__init__()\n",
        "        # shared MLP\n",
        "        self.fc1=nn.Linear(z_dim, hidden)\n",
        "        self.fc2=nn.Linear(hidden, z_dim)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.fc1.bias); nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "        self.z_dim=z_dim; self.hidden=hidden\n",
        "        self.experts=nn.ModuleList([ExpertLoRA(z_dim,hidden,adapters_per_expert,rank) for _ in range(init_experts)])\n",
        "        self.prototypes=nn.Parameter(torch.randn(init_experts, z_dim)*0.02)\n",
        "        self.adapter_keys=nn.Parameter(torch.randn(init_experts, adapters_per_expert, z_dim)*0.02)\n",
        "        self.logit_bias=nn.Parameter(torch.zeros(init_experts))\n",
        "\n",
        "    def num_experts(self): return len(self.experts)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def add_expert(self, clone_from: Optional[List[int]]=None, donors_fraction: float=0.7, noise_std: float=0.02, bias_boost: float=0.0):\n",
        "        \"\"\"\n",
        "        Create a new Expert on SAME device/dtype as existing params.\n",
        "        70% of adapter rows cloned from donors (+noise), 30% random init.\n",
        "        \"\"\"\n",
        "        dev = self.prototypes.device\n",
        "        dt  = self.prototypes.dtype\n",
        "        W=cfg.adapters_per_expert; D=self.z_dim; H=self.hidden; R=cfg.lora_rank\n",
        "\n",
        "        new = ExpertLoRA(D,H,W,R).to(device=dev, dtype=dt)\n",
        "\n",
        "        if clone_from:\n",
        "            donors = [d for d in clone_from if 0<=d<self.num_experts()]\n",
        "            if len(donors)>0:\n",
        "                take = int(W*donors_fraction)\n",
        "                idxs = torch.randperm(W, device=dev)[:take]\n",
        "                donor_sel = torch.randint(0, len(donors), (take,), device=dev)\n",
        "                for wrow,di in zip(idxs.tolist(), donor_sel.tolist()):\n",
        "                    d = donors[di]\n",
        "                    new.a1[wrow].copy_( self.experts[d].a1[wrow].to(dev, dt) + noise_std*torch.randn_like(new.a1[wrow], device=dev, dtype=dt) )\n",
        "                    new.b1[wrow].copy_( self.experts[d].b1[wrow].to(dev, dt) + noise_std*torch.randn_like(new.b1[wrow], device=dev, dtype=dt) )\n",
        "                    new.a2[wrow].copy_( self.experts[d].a2[wrow].to(dev, dt) + noise_std*torch.randn_like(new.a2[wrow], device=dev, dtype=dt) )\n",
        "                    new.b2[wrow].copy_( self.experts[d].b2[wrow].to(dev, dt) + noise_std*torch.randn_like(new.b2[wrow], device=dev, dtype=dt) )\n",
        "                proto = self.prototypes.data[donors].mean(dim=0).to(dev, dt).clone()\n",
        "                keys  = self.adapter_keys.data[donors].mean(dim=0).to(dev, dt).clone()\n",
        "            else:\n",
        "                proto=torch.randn(D, device=dev, dtype=dt)*noise_std\n",
        "                keys=torch.randn(W, D, device=dev, dtype=dt)*noise_std\n",
        "        else:\n",
        "            proto=torch.randn(D, device=dev, dtype=dt)*noise_std\n",
        "            keys=torch.randn(W, D, device=dev, dtype=dt)*noise_std\n",
        "\n",
        "        self.experts.append(new)\n",
        "        self.prototypes = nn.Parameter(torch.cat([self.prototypes.data, proto[None,:]], dim=0).to(dev, dt))\n",
        "        self.adapter_keys = nn.Parameter(torch.cat([self.adapter_keys.data, keys[None,:,:]], dim=0).to(dev, dt))\n",
        "        self.logit_bias = nn.Parameter(torch.cat([self.logit_bias.data, torch.tensor([bias_boost], device=dev, dtype=dt)], dim=0))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def remove_experts(self, remove_idx: List[int]):\n",
        "        if not remove_idx: return 0\n",
        "        E=self.num_experts()\n",
        "        keep = sorted(set(i for i in range(E) if i not in set(remove_idx)))\n",
        "        if len(keep)==E: return 0\n",
        "        new_list=nn.ModuleList([self.experts[i] for i in keep])\n",
        "        self.experts = new_list\n",
        "        dev=self.prototypes.device; dt=self.prototypes.dtype\n",
        "        self.prototypes = nn.Parameter(self.prototypes.data[keep].clone().to(dev, dt))\n",
        "        self.adapter_keys = nn.Parameter(self.adapter_keys.data[keep].clone().to(dev, dt))\n",
        "        self.logit_bias = nn.Parameter(self.logit_bias.data[keep].clone().to(dev, dt))\n",
        "        return E - len(keep)\n",
        "\n",
        "    def capacity_route(self, probs, top_k, capacity):\n",
        "        B,E=probs.shape\n",
        "        k=min(top_k, E)\n",
        "        cand_k=min(E, max(k*4, k))\n",
        "        vals, idxs=torch.topk(probs, k=cand_k, dim=-1)\n",
        "        assigned_idx=torch.full((B,k), -1, dtype=torch.long, device=probs.device)\n",
        "        assigned_w=torch.zeros(B,k, device=probs.device, dtype=probs.dtype)\n",
        "        cap=torch.zeros(E, dtype=torch.int32, device=probs.device)\n",
        "        cap_limit=torch.full((E,), max(1,capacity), dtype=torch.int32, device=probs.device)\n",
        "        cap_hit=0; total_slots=B*k\n",
        "        for b in range(B):\n",
        "            slot=0\n",
        "            for j in range(cand_k):\n",
        "                if slot>=k: break\n",
        "                e=int(idxs[b,j].item())\n",
        "                if cap[e] < cap_limit[e]:\n",
        "                    assigned_idx[b,slot]=e\n",
        "                    assigned_w[b,slot]=vals[b,j]\n",
        "                    cap[e]+=1\n",
        "                    if j>0: cap_hit+=1\n",
        "                    slot+=1\n",
        "        cap_hit_rate = cap_hit/max(1,total_slots)\n",
        "        return assigned_idx, assigned_w, cap_hit_rate\n",
        "\n",
        "    def pick_adapters(self, z, expert_ids):\n",
        "        N=z.size(0); dt=z.dtype\n",
        "        a_idx=torch.zeros(N, dtype=torch.long, device=z.device)\n",
        "        for u in expert_ids.unique():\n",
        "            u=int(u.item())\n",
        "            sel=(expert_ids==u)\n",
        "            logits = z[sel] @ self.adapter_keys[u].to(dt).t()\n",
        "            a_idx[sel]=torch.argmax(logits, dim=-1)\n",
        "        return a_idx\n",
        "\n",
        "    def forward(self, z, top_k, tau, epsilon, capacity, ban_expert: Optional[int]=None):\n",
        "        h_pre = F.silu(self.fc1(z))\n",
        "        out_pre = self.fc2(h_pre)\n",
        "\n",
        "        logits=F.linear(z, self.prototypes) / max(tau,1e-6)\n",
        "        logits = logits + self.logit_bias\n",
        "        if ban_expert is not None and 0<=ban_expert<self.num_experts():\n",
        "            logits[:, ban_expert] = -1e9\n",
        "        probs=F.softmax(logits, dim=-1)\n",
        "        if epsilon>0:\n",
        "            probs=(1-epsilon)*probs + epsilon*(torch.ones_like(probs)/probs.size(1))\n",
        "\n",
        "        assigned_idx, assigned_w, cap_hit_rate = self.capacity_route(probs, top_k, capacity)\n",
        "        B,E=probs.shape; k=assigned_idx.size(1)\n",
        "        z_out=torch.zeros_like(z)\n",
        "        used_mask=torch.zeros(B,E, dtype=torch.bool, device=z.device)\n",
        "\n",
        "        for j in range(k):\n",
        "            idx_j=assigned_idx[:,j]; w_j=assigned_w[:,j].unsqueeze(1).to(z.dtype)\n",
        "            sel=(idx_j>=0)\n",
        "            if not sel.any(): continue\n",
        "            idx_j_sel=idx_j[sel]; z_sel=z[sel]; h_sel=h_pre[sel]; out_sel=out_pre[sel]\n",
        "            for u in idx_j_sel.unique():\n",
        "                u=int(u.item())\n",
        "                sub=(idx_j_sel==u)\n",
        "                z_sub=z_sel[sub]; h_sub=h_sel[sub]; out_sub=out_sel[sub]\n",
        "                a_sub=self.pick_adapters(z_sub, torch.full((z_sub.size(0),), u, dtype=torch.long, device=z.device))\n",
        "                y=self.experts[u].forward_batch(z_sub, a_sub, h_sub, out_sub).to(z.dtype)\n",
        "                b_idx=torch.nonzero(sel, as_tuple=False).squeeze(1)[sub]\n",
        "                z_out[b_idx] = z_out[b_idx] + w_j[sel][sub] * y\n",
        "                used_mask[b_idx, u]=True\n",
        "\n",
        "        return z + z_out, probs, used_mask, assigned_idx, assigned_w, cap_hit_rate\n",
        "\n",
        "class AutoEncoderMoE(nn.Module):\n",
        "    def __init__(self, init_E: int):\n",
        "        super().__init__()\n",
        "        self.enc=Encoder(cfg.z_dim)\n",
        "        self.moe=MoELatent(cfg.z_dim, cfg.hidden, init_E, cfg.adapters_per_expert, cfg.lora_rank)\n",
        "        self.dec=Decoder(cfg.z_dim)\n",
        "    def forward(self, x, top_k, tau, epsilon, capacity, ban_expert: Optional[int]=None):\n",
        "        z=self.enc(x)\n",
        "        z_moe, probs, used_mask, aidx, aw, cap_hit = self.moe(z, top_k, tau, epsilon, capacity, ban_expert=ban_expert)\n",
        "        x_rec=self.dec(z_moe)\n",
        "        return x_rec, z, z_moe, probs, used_mask, aidx, aw, cap_hit\n",
        "\n",
        "def denorm(x): return (x.clamp(-1,1)+1)*0.5\n",
        "\n",
        "# ---------------- Loss helpers ----------------\n",
        "def gate_entropy(probs):\n",
        "    p=probs.clamp_min(1e-8)\n",
        "    return (-p * p.log()).sum(dim=-1).mean()\n",
        "\n",
        "def repulsion_loss(protos, m=256, sigma=0.15):\n",
        "    E=protos.size(0)\n",
        "    if E<=1: return protos.sum()*0\n",
        "    idx=torch.randperm(E, device=protos.device)[:min(m,E)]\n",
        "    P=F.normalize(protos[idx], dim=-1)\n",
        "    S=P @ P.t()\n",
        "    mask=~torch.eye(P.size(0), device=P.device, dtype=torch.bool)\n",
        "    s=S[mask]\n",
        "    return torch.exp(s/sigma).mean()\n",
        "\n",
        "# ---------------- Economy & Pattern Board ----------------\n",
        "class ExpertStats:\n",
        "    def __init__(self, E:int):\n",
        "        self.reset(E)\n",
        "    def reset(self,E):\n",
        "        dev=device\n",
        "        self.E=E\n",
        "        self.usage_epoch=torch.zeros(E, dtype=torch.float32, device=dev)\n",
        "        self.usage_ema  =torch.zeros(E, dtype=torch.float32, device=dev)\n",
        "        self.contrib_ema=torch.zeros(E, dtype=torch.float32, device=dev)\n",
        "        self.credit     =torch.zeros(E, dtype=torch.float32, device=dev)\n",
        "        self.neg_streak =torch.zeros(E, dtype=torch.int32 , device=dev)\n",
        "    def on_structure_add(self, new_added:int):\n",
        "        if new_added<=0: return\n",
        "        dev=device\n",
        "        self.E += new_added\n",
        "        self.usage_epoch = torch.cat([self.usage_epoch, torch.zeros(new_added, device=dev)])\n",
        "        self.usage_ema   = torch.cat([self.usage_ema  , torch.zeros(new_added, device=dev)])\n",
        "        self.contrib_ema = torch.cat([self.contrib_ema, torch.zeros(new_added, device=dev)])\n",
        "        self.credit      = torch.cat([self.credit     , torch.zeros(new_added, device=dev)])\n",
        "        self.neg_streak  = torch.cat([self.neg_streak , torch.zeros(new_added, dtype=torch.int32, device=dev)])\n",
        "    def on_structure_remove(self, keep_idx: List[int]):\n",
        "        dev=device\n",
        "        keep_idx=torch.tensor(keep_idx, dtype=torch.long, device=dev)\n",
        "        self.E = keep_idx.numel()\n",
        "        self.usage_epoch = self.usage_epoch[keep_idx]\n",
        "        self.usage_ema   = self.usage_ema[keep_idx]\n",
        "        self.contrib_ema = self.contrib_ema[keep_idx]\n",
        "        self.credit      = self.credit[keep_idx]\n",
        "        self.neg_streak  = self.neg_streak[keep_idx]\n",
        "    def epoch_decay(self, decay):\n",
        "        self.usage_ema = decay*self.usage_ema + (1-decay)*self.usage_epoch\n",
        "        self.usage_epoch.zero_()\n",
        "\n",
        "class EWMA:\n",
        "    def __init__(self, beta=0.98):\n",
        "        self.beta=beta; self.mean=None; self.var=None\n",
        "    def update(self, x: float):\n",
        "        x=float(x)\n",
        "        if self.mean is None:\n",
        "            self.mean=x; self.var=1e-6\n",
        "        else:\n",
        "            m=self.mean; v=self.var\n",
        "            m_new = self.beta*m + (1-self.beta)*x\n",
        "            v_new = self.beta*v + (1-self.beta)*(x-m_new)*(x-m)+1e-9\n",
        "            self.mean=m_new; self.var=v_new\n",
        "    def normz(self, x):\n",
        "        if self.mean is None: return 0.0\n",
        "        return (float(x)-self.mean)/max(1e-8, math.sqrt(self.var))\n",
        "\n",
        "class PatternCandidate:\n",
        "    def __init__(self, signature, reporter_eid, S):\n",
        "        self.signature=signature\n",
        "        self.reporters=set([reporter_eid])\n",
        "        self.S_list=[S]\n",
        "        self.windows=1\n",
        "        self.hits=1\n",
        "        self.state=\"DETECTED\"  # DETECTED -> REVIEW -> APPROVED/DECLINED\n",
        "        self.created_step=0\n",
        "        self.last_update_step=0\n",
        "\n",
        "class PatternBoard:\n",
        "    def __init__(self):\n",
        "        self.S_ewma=EWMA(0.98)\n",
        "        self.rec_ewma=EWMA(0.98)\n",
        "        self.js_ewma=EWMA(0.98)\n",
        "        self.sig_store=[]\n",
        "        self.candidates: List[PatternCandidate]=[]\n",
        "        self.step=0\n",
        "        self.approved=0; self.declined=0\n",
        "        self.cluster_centers=None  # float32\n",
        "        self.cluster_counts=None\n",
        "    def signature(self, z_bar, p_bar):\n",
        "        v=torch.cat([F.normalize(z_bar.float(), dim=0), F.normalize(p_bar.float(), dim=0)], dim=0)\n",
        "        v = F.normalize(v, dim=0)\n",
        "        return v.detach()\n",
        "    @staticmethod\n",
        "    def js_div(p, q, eps=1e-8):\n",
        "        p = p.float(); q = q.float()\n",
        "        m=0.5*(p+q)\n",
        "        kl1=(p.clamp_min(eps)*(p.clamp_min(eps)/m.clamp_min(eps)).log()).sum(dim=-1)\n",
        "        kl2=(q.clamp_min(eps)*(q.clamp_min(eps)/m.clamp_min(eps)).log()).sum(dim=-1)\n",
        "        return 0.5*(kl1+kl2)\n",
        "    def update_clusters(self, z_bar):\n",
        "        with torch.no_grad():\n",
        "            z = z_bar.detach().float()\n",
        "            if self.cluster_centers is None:\n",
        "                self.cluster_centers = z[None,:].to(device=device, dtype=torch.float32)\n",
        "                self.cluster_counts  = torch.ones(1, dtype=torch.float32, device=device)\n",
        "                return 0.0\n",
        "            centers = self.cluster_centers\n",
        "            diffs = centers - z.unsqueeze(0)\n",
        "            dists = torch.linalg.vector_norm(diffs, dim=1)\n",
        "            k=int(torch.argmin(dists).item())\n",
        "            dmin=float(dists[k].item())\n",
        "            c=centers[k]; n=self.cluster_counts[k]\n",
        "            alpha=1.0/(n+1.0)\n",
        "            self.cluster_centers[k] = (1-alpha)*c + alpha*z   # <- fixed here\n",
        "            self.cluster_counts[k]  = n+1.0\n",
        "            if dmin>2.0 and self.cluster_centers.size(0)<16:\n",
        "                self.cluster_centers = torch.cat([self.cluster_centers, z[None,:]], dim=0)\n",
        "                self.cluster_counts  = torch.cat([self.cluster_counts, torch.ones(1, device=device)], dim=0)\n",
        "            return dmin\n",
        "\n",
        "# ---------------- Helpers ----------------\n",
        "def compute_capacity(B, E, k):\n",
        "    cap = int(math.ceil(cfg.capacity_alpha * B * k / max(1,E)))\n",
        "    return max(1, cap)\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_samples(epoch, model):\n",
        "    model.eval()\n",
        "    imgs=next(iter(val_loader))[:cfg.sample_rows*cfg.sample_rows].to(device)\n",
        "    capacity=compute_capacity(imgs.size(0), model.moe.num_experts(), cfg.top_k)\n",
        "    rec, *_ = model(imgs, top_k=cfg.top_k, tau=1.0, epsilon=0.0, capacity=capacity)\n",
        "    grid=make_grid(torch.cat([denorm(imgs), denorm(rec)], dim=0), nrow=cfg.sample_rows)\n",
        "    save_image(grid, os.path.join(cfg.out_samples_dir, f\"epoch_{epoch:03d}.png\"))\n",
        "    model.train()\n",
        "\n",
        "def summarize_epoch(epoch, model, stats, train_loss, val_loss, share, hhi, entropy_mean, cap_hit_rate, approved, declined):\n",
        "    E=model.moe.num_experts()\n",
        "    usage=stats.usage_ema.detach().cpu()\n",
        "    credit=stats.credit.detach().cpu()\n",
        "    k=min(5,E)\n",
        "    if usage.sum()>0 and k>0:\n",
        "        share_vec=(usage/usage.sum())\n",
        "        s_vals, s_idx=torch.topk(share_vec, k=k)\n",
        "        top5_share=[(int(s_idx[j]), float(s_vals[j])) for j in range(k)]\n",
        "    else:\n",
        "        top5_share=[]\n",
        "    if k>0:\n",
        "        c_vals, c_idx=torch.topk(credit, k=k)\n",
        "        top5_credit=[(int(c_idx[j]), float(c_vals[j])) for j in range(k)]\n",
        "    else:\n",
        "        top5_credit=[]\n",
        "    cov = (usage>0).float().sum().item()\n",
        "\n",
        "    print(f\"\\n[Epoch {epoch}] Experts={E} | NeuronScale={E*cfg.hidden} | Coverage={cov}/{E} | \"\n",
        "          f\"HHI={hhi:.4f} | Entropy={entropy_mean:.3f} | CapHit={cap_hit_rate:.3f} | \"\n",
        "          f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | \"\n",
        "          f\"Patterns: approved={approved} declined={declined}\")\n",
        "    print(\"Top-5 by SHARE (id, share):\")\n",
        "    for eid,s in top5_share: print(f\"  - {eid:4d} | {s:.4f}\")\n",
        "    print(\"Top-5 by CREDIT (id, credit):\")\n",
        "    for eid,c in top5_credit: print(f\"  - {eid:4d} | {c:.3f}\")\n",
        "\n",
        "    with open(os.path.join(cfg.out_ckpt_dir, f\"epoch_{epoch:03d}_summary.json\"),\"w\") as f:\n",
        "        json.dump({\n",
        "            \"epoch\": epoch,\n",
        "            \"experts_total\": E,\n",
        "            \"neuron_scale\": E*cfg.hidden,\n",
        "            \"coverage\": cov,\n",
        "            \"hhi\": hhi,\n",
        "            \"entropy_mean\": entropy_mean,\n",
        "            \"capacity_hit_rate\": cap_hit_rate,\n",
        "            \"top5_share\":[{\"expert_id\":eid,\"share\":s} for eid,s in top5_share],\n",
        "            \"top5_credit\":[{\"expert_id\":eid,\"credit\":c} for eid,c in top5_credit],\n",
        "            \"train_loss\": float(train_loss),\n",
        "            \"val_loss\": float(val_loss),\n",
        "            \"patterns\":{\"approved\":approved,\"declined\":declined},\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        }, f, indent=2)\n",
        "\n",
        "def share_hhi_from_usage(usage):\n",
        "    tot=usage.sum().item()+1e-6\n",
        "    s=usage/tot\n",
        "    hhi=float((s*s).sum().item())\n",
        "    return s, hhi\n",
        "\n",
        "@torch.no_grad()\n",
        "def loo_probe_contrib(model, stats, tau, capacity, percept):\n",
        "    if len(val_loader)==0: return\n",
        "    batch=next(iter(val_loader))[:64].to(device)\n",
        "    model.eval()\n",
        "    with torch.amp.autocast(\"cuda\", enabled=cfg.amp):\n",
        "        x_rec, *_ = model(batch, top_k=cfg.top_k, tau=tau, epsilon=0.0, capacity=capacity)\n",
        "        base = F.mse_loss(x_rec,batch)\n",
        "        if percept is not None:\n",
        "            base = 0.7*base + 0.3*percept(denorm(x_rec).float(), denorm(batch).float()).mean()\n",
        "    usage=stats.usage_ema\n",
        "    k=min(16, model.moe.num_experts())\n",
        "    if usage.sum()>0 and k>0:\n",
        "        _, top_idx=torch.topk(usage, k=k)\n",
        "    else:\n",
        "        model.train(); return\n",
        "    for e in top_idx.tolist():\n",
        "        with torch.amp.autocast(\"cuda\", enabled=cfg.amp):\n",
        "            x_loo, *_ = model(batch, top_k=cfg.top_k, tau=tau, epsilon=0.0, capacity=capacity, ban_expert=int(e))\n",
        "            loss_loo = F.mse_loss(x_loo,batch)\n",
        "        dL=float((loss_loo - base).item())\n",
        "        stats.contrib_ema[e] = 0.9*stats.contrib_ema[e] + 0.1*torch.tensor(max(0.0, dL), device=device)\n",
        "    model.train()\n",
        "\n",
        "# ---------------- Build model (OOM-aware) ----------------\n",
        "def build_model_oom_aware(desired_E: int):\n",
        "    E = max(8, desired_E)\n",
        "    while True:\n",
        "        try:\n",
        "            model = AutoEncoderMoE(init_E=E).to(device)\n",
        "            stats  = ExpertStats(E)\n",
        "            print(f\"[Model] Built with E={E} experts\")\n",
        "            return model, stats\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower() and E>8:\n",
        "                print(f\"[Model] OOM at E={E}. Trying E={E//2} â€¦\")\n",
        "                torch.cuda.empty_cache()\n",
        "                E = max(8, E//2)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "model, stats = build_model_oom_aware(cfg.init_experts_desired)\n",
        "percept = lpips.LPIPS(net='vgg').to(device).eval() if _HAS_LPIPS else None\n",
        "\n",
        "# SGD optimizer\n",
        "opt = torch.optim.SGD(model.parameters(), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay)\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=cfg.amp)\n",
        "\n",
        "# ---------------- Schedules ----------------\n",
        "def tau_sched(epoch):\n",
        "    if epoch<=5: return cfg.tau_high\n",
        "    t=(epoch-5)/max(1,(cfg.epochs-5))\n",
        "    return cfg.tau_high + (cfg.tau_low-cfg.tau_high)*t\n",
        "def eps_sched(epoch):\n",
        "    t=(epoch-1)/max(1,(cfg.epochs-1))\n",
        "    return cfg.epsilon_start + (cfg.epsilon_end-cfg.epsilon_start)*t\n",
        "def ent_lambda_sched(epoch):\n",
        "    return cfg.gate_entropy_lambda_warm if epoch<=5 else cfg.gate_entropy_lambda_cool\n",
        "\n",
        "# ---------------- Pattern Board (simple; only novelty used) ----------------\n",
        "class SimpleBoard:\n",
        "    def __init__(self):\n",
        "        self.approved=0; self.declined=0\n",
        "        self.S_ema=None; self.beta=0.98\n",
        "    def upd(self, S):\n",
        "        if self.S_ema is None: self.S_ema=S\n",
        "        else: self.S_ema=self.beta*self.S_ema+(1-self.beta)*S\n",
        "    def norm(self,S):\n",
        "        if self.S_ema is None: return 0.0\n",
        "        return S - self.S_ema\n",
        "\n",
        "board=SimpleBoard()\n",
        "\n",
        "# ---------------- Training ----------------\n",
        "global_step=0\n",
        "p_avg=None  # EMA of gate distribution\n",
        "\n",
        "for epoch in range(1, cfg.epochs+1):\n",
        "    model.train()\n",
        "    tau=tau_sched(epoch); eps=eps_sched(epoch); ent_l=ent_lambda_sched(epoch)\n",
        "\n",
        "    running=0.0; cap_hits=0.0; cap_tot=0.0; ent_sum=0.0; ent_cnt=0\n",
        "    pbar=tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg.epochs} (train)\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        batch=batch.to(device, non_blocking=True)\n",
        "        B=batch.size(0)\n",
        "        capacity=compute_capacity(B, model.moe.num_experts(), cfg.top_k)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=cfg.amp):\n",
        "            x_rec, z0, z_moe, probs, used_mask, aidx, aw, cap_hit = model(\n",
        "                batch, top_k=cfg.top_k, tau=tau, epsilon=eps, capacity=capacity\n",
        "            )\n",
        "            mse=F.mse_loss(x_rec, batch)\n",
        "            loss_main = mse\n",
        "            if _HAS_LPIPS:\n",
        "                lp = percept(denorm(x_rec).float(), denorm(batch).float()).mean()\n",
        "                loss_main = 0.7*mse + 0.3*lp\n",
        "            H=gate_entropy(probs)\n",
        "            rep=repulsion_loss(model.moe.prototypes, m=cfg.repulsion_subset, sigma=cfg.repulsion_sigma)\n",
        "            loss = loss_main + ent_l*H + cfg.repulsion_lambda*rep\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(opt)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(opt); scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            E=model.moe.num_experts()\n",
        "            stats.usage_epoch[:E]+=used_mask.float().sum(dim=0)\n",
        "            cap_hits += cap_hit; cap_tot+=1.0\n",
        "            ent_sum  += float(H.item()); ent_cnt+=1\n",
        "\n",
        "            # novelty S (lower â†’ easier approval)\n",
        "            p_bar = probs.mean(dim=0)\n",
        "            if p_avg is None: p_avg = p_bar.detach()\n",
        "            js = float((0.5*(p_bar.clamp_min(1e-8)*((p_bar.clamp_min(1e-8)/(0.5*(p_bar+p_avg)).clamp_min(1e-8))).log()).sum() +\n",
        "                        0.5*(p_avg.clamp_min(1e-8)*((p_avg.clamp_min(1e-8)/(0.5*(p_bar+p_avg)).clamp_min(1e-8))).log()).sum()).item())\n",
        "            p_avg = 0.98*p_avg + 0.02*p_bar.detach()\n",
        "            z_bar = z0.mean(dim=0)\n",
        "            dmin = float((model.moe.prototypes - z_bar).pow(2).sum(dim=1).sqrt().min().item())\n",
        "            S = 0.50*dmin + 0.30*max(0.0, js) + 0.20*float(H.item())\n",
        "            board.upd(S)\n",
        "            Sn = board.norm(S)\n",
        "\n",
        "            # approve if S >= threshold (single reporter)\n",
        "            if S >= cfg.report_theta:\n",
        "                board.approved += 1\n",
        "                # recruit\n",
        "                share_vec = (stats.usage_ema / (stats.usage_ema.sum()+1e-6))\n",
        "                E = model.moe.num_experts()\n",
        "                top_vals, top_idx = torch.topk(share_vec, k=min(cfg.big_topN, E))\n",
        "                enterprises = set([int(i) for i in top_idx.tolist()])\n",
        "                enterprises |= set([int(i) for i,v in enumerate(share_vec.tolist()) if v>=cfg.big_share_thr])\n",
        "                donor_pool = list(enterprises) if len(enterprises)>0 else [int(torch.argmax(used_mask.float().sum(dim=0)).item())]\n",
        "                Savg=S\n",
        "                Q = int(cfg.team_base_Q * (1.0 + cfg.team_alpha_S*max(0.0, Savg-cfg.report_theta)))\n",
        "                new_expert_count = max(1, Q // cfg.adapters_per_expert)\n",
        "                for _ in range(new_expert_count):\n",
        "                    k_choose = max(1, min(len(donor_pool), max(1, len(donor_pool)//2)))\n",
        "                    donors = random.sample(donor_pool, k=k_choose)\n",
        "                    model.moe.add_expert(clone_from=donors, donors_fraction=0.7, bias_boost=cfg.recruit_new_bias)\n",
        "                    stats.on_structure_add(new_added=1)\n",
        "                with torch.no_grad():\n",
        "                    for d in donor_pool:\n",
        "                        model.moe.logit_bias.data[d] += cfg.recruit_donor_bias\n",
        "                print(f\"[Growth] Approved pattern S={S:.2f} â†’ +{new_expert_count} experts (E={model.moe.num_experts()})\")\n",
        "\n",
        "        running+=float(loss_main.item())\n",
        "        pbar.set_postfix(loss=f\"{loss_main.item():.4f}\", mse=f\"{mse.item():.4f}\", H=f\"{H.item():.3f}\",\n",
        "                         tau=f\"{tau:.2f}\", eps=f\"{eps:.3f}\", S=f\"{S:.2f}\", Sn=f\"{Sn:.2f}\")\n",
        "        global_step+=1\n",
        "\n",
        "    # end epoch usage decay\n",
        "    stats.epoch_decay(decay=0.9)\n",
        "\n",
        "    # Validation\n",
        "    model.eval(); val_loss=0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch}/{cfg.epochs} (val)\"):\n",
        "            batch=batch.to(device, non_blocking=True)\n",
        "            capacity=compute_capacity(batch.size(0), model.moe.num_experts(), cfg.top_k)\n",
        "            with torch.amp.autocast(\"cuda\", enabled=cfg.amp):\n",
        "                x_rec, *_ = model(batch, top_k=cfg.top_k, tau=tau, epsilon=0.0, capacity=capacity)\n",
        "                loss_v = F.mse_loss(x_rec,batch)\n",
        "                if _HAS_LPIPS:\n",
        "                    lp = percept(denorm(x_rec).float(), denorm(batch).float()).mean()\n",
        "                    loss_v = 0.7*loss_v + 0.3*lp\n",
        "            val_loss += float(loss_v.item())*batch.size(0)\n",
        "    val_loss/=max(1,len(val_ds))\n",
        "\n",
        "    # LOO probe (light)\n",
        "    loo_probe_contrib(model, stats, tau=tau, capacity=compute_capacity(64, model.moe.num_experts(), cfg.top_k), percept=percept)\n",
        "\n",
        "    # Share/HHI\n",
        "    share_vec, hhi = share_hhi_from_usage(stats.usage_ema)\n",
        "    entropy_mean = ent_sum/max(1,ent_cnt)\n",
        "    cap_hit_rate = cap_hits/max(1,cap_tot)\n",
        "\n",
        "    # --------- Credit settle ----------\n",
        "    with torch.no_grad():\n",
        "        approved_reward = cfg.credit_true_reward * float(board.approved)\n",
        "        declined_cost  = cfg.credit_false_penalty * float(board.declined)\n",
        "        stats.credit += torch.tensor(approved_reward - declined_cost, device=device) * (share_vec / (share_vec.sum()+1e-6))\n",
        "        stats.credit += cfg.credit_contrib_scale * stats.contrib_ema\n",
        "        div_weight = (1.0 - (share_vec**2))\n",
        "        stats.credit += cfg.credit_diversity_scale * float(max(0.0, entropy_mean)) * div_weight\n",
        "        over = (share_vec - cfg.share_tax_thr).clamp_min(0.0)\n",
        "        stats.credit -= cfg.share_tax_lambda * over\n",
        "        stats.credit *= cfg.credit_decay\n",
        "        neg_mask = (stats.credit<0)\n",
        "        stats.credit[neg_mask] = stats.credit[neg_mask] * (1.0 + cfg.neg_interest)\n",
        "        stats.neg_streak[stats.credit<=-cfg.bankrupcy_level] += 1\n",
        "        stats.neg_streak[stats.credit>-cfg.bankrupcy_level] = 0\n",
        "\n",
        "    # --------- PRUNE ----------\n",
        "    if cfg.enable_prune:\n",
        "        with torch.no_grad():\n",
        "            E=model.moe.num_experts()\n",
        "            if E>cfg.prune_min_keep:\n",
        "                usage = stats.usage_ema\n",
        "                share = (usage/(usage.sum()+1e-6))\n",
        "                cand_mask = (usage <= cfg.prune_usage_eps) & (stats.credit <= -cfg.prune_credit_level) & (stats.neg_streak >= cfg.bankrupcy_streak)\n",
        "                cand_idx = torch.nonzero(cand_mask, as_tuple=False).view(-1).tolist()\n",
        "                top_keep = torch.topk(share, k=min(64,E)).indices.tolist()\n",
        "                cand_idx = [i for i in cand_idx if i not in set(top_keep)]\n",
        "                max_prune = min(cfg.prune_max_per_epoch, max(0, E - cfg.prune_min_keep))\n",
        "                cand_idx = cand_idx[:max_prune]\n",
        "                if cand_idx:\n",
        "                    keep = [i for i in range(E) if i not in set(cand_idx)]\n",
        "                    removed = model.moe.remove_experts(cand_idx)\n",
        "                    stats.on_structure_remove(keep)\n",
        "                    print(f\"[Structure] Pruned {removed} experts â†’ E={model.moe.num_experts()}\")\n",
        "\n",
        "    # Save sample + ckpt\n",
        "    save_samples(epoch, model)\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": opt.state_dict(),\n",
        "        \"stats\": {\n",
        "            \"usage_ema\": stats.usage_ema.detach().cpu().tolist(),\n",
        "            \"contrib_ema\": stats.contrib_ema.detach().cpu().tolist(),\n",
        "            \"credit\": stats.credit.detach().cpu().tolist(),\n",
        "            \"neg_streak\": stats.neg_streak.detach().cpu().tolist(),\n",
        "        },\n",
        "        \"config\": cfg.__dict__,\n",
        "    }, os.path.join(cfg.out_ckpt_dir, f\"epoch_{epoch:03d}.pt\"))\n",
        "\n",
        "    # Epoch summary\n",
        "    summarize_epoch(epoch, model, stats,\n",
        "                    train_loss=running/len(train_loader), val_loss=val_loss,\n",
        "                    share=share_vec, hhi=hhi, entropy_mean=entropy_mean, cap_hit_rate=cap_hit_rate,\n",
        "                    approved=board.approved, declined=board.declined)\n",
        "\n",
        "print(\"âœ… Training complete. See ./checkpoints & ./samples\")\n"
      ]
    }
  ]
}