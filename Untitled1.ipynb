{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM4jn/2IjhSLh4GvVzSdXOH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93717d5356e043f3a97e799e55293d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22f53d0c518f4a068ce2a6a842080a05",
              "IPY_MODEL_d95448b08d2141d5af2859912606d813",
              "IPY_MODEL_565c23b6ba0842908f1fc1b7305786ef"
            ],
            "layout": "IPY_MODEL_25e690c6a63a48949b36810dc7dc4694"
          }
        },
        "22f53d0c518f4a068ce2a6a842080a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a06e7dd3065a492e973b7823f58f5ec2",
            "placeholder": "​",
            "style": "IPY_MODEL_b4514188037640efacdd501ff9a06379",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "d95448b08d2141d5af2859912606d813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_963e4b227e804dcd87a9dfa7eb76da07",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_049fd854e39b4f08b76d76b57849a676",
            "value": 5
          }
        },
        "565c23b6ba0842908f1fc1b7305786ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7648aeed343b49a293d05968173b1f3e",
            "placeholder": "​",
            "style": "IPY_MODEL_cb4a8e46183f45ecbd0bdec54bf38f93",
            "value": " 5/5 [00:00&lt;00:00,  4.43it/s]"
          }
        },
        "25e690c6a63a48949b36810dc7dc4694": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06e7dd3065a492e973b7823f58f5ec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4514188037640efacdd501ff9a06379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "963e4b227e804dcd87a9dfa7eb76da07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "049fd854e39b4f08b76d76b57849a676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7648aeed343b49a293d05968173b1f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb4a8e46183f45ecbd0bdec54bf38f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/username12345678901234567890/1234/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zN2BsopaR4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "93717d5356e043f3a97e799e55293d2e",
            "22f53d0c518f4a068ce2a6a842080a05",
            "d95448b08d2141d5af2859912606d813",
            "565c23b6ba0842908f1fc1b7305786ef",
            "25e690c6a63a48949b36810dc7dc4694",
            "a06e7dd3065a492e973b7823f58f5ec2",
            "b4514188037640efacdd501ff9a06379",
            "963e4b227e804dcd87a9dfa7eb76da07",
            "049fd854e39b4f08b76d76b57849a676",
            "7648aeed343b49a293d05968173b1f3e",
            "cb4a8e46183f45ecbd0bdec54bf38f93"
          ]
        },
        "outputId": "ad21a4ac-3dde-454f-b289-764d8e09d389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93717d5356e043f3a97e799e55293d2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
            "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
            "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
            "<ipython-input-3-f3a256382578>:89: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-3-f3a256382578>:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  pixel_values = torch.tensor(torch.ByteTensor(torch.ByteStorage.from_buffer(image.tobytes())).float()).view(image.size[1], image.size[0], 3).permute(2, 0, 1) / 255.0\n",
            "<ipython-input-3-f3a256382578>:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pixel_values = torch.tensor(torch.ByteTensor(torch.ByteStorage.from_buffer(image.tobytes())).float()).view(image.size[1], image.size[0], 3).permute(2, 0, 1) / 255.0\n",
            "<ipython-input-3-f3a256382578>:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# %% [single-cell train] Plasticity 기반 Self-Organizing MoE - 실전용 (Colab A100 40GB)\n",
        "# ---------------------------------------\n",
        "# 설정\n",
        "# ---------------------------------------\n",
        "import os, sys, json, time, glob, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
        "os.makedirs(\"./samples\", exist_ok=True)\n",
        "\n",
        "# Colab 등에서 LPIPS 설치 (없으면 건너뜀)\n",
        "try:\n",
        "    import lpips  # noqa\n",
        "    _HAS_LPIPS = True\n",
        "except Exception:\n",
        "    try:\n",
        "        # 인터넷이 되지 않는 환경이면 자동으로 실패 -> MSE만 사용\n",
        "        !pip -q install lpips==0.1.4\n",
        "        import lpips  # noqa\n",
        "        _HAS_LPIPS = True\n",
        "    except Exception:\n",
        "        _HAS_LPIPS = False\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------------------------------\n",
        "# Config\n",
        "# ---------------------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_root: str = \"/images\"            # 이미지 폴더 (하위 폴더 포함)\n",
        "    image_size: int = 128\n",
        "    epochs: int = 20\n",
        "    batch_size: int = 192                 # A100 40GB 기준 권장\n",
        "    num_workers: int = 4\n",
        "    lr: float = 2e-4\n",
        "    betas: Tuple[float, float] = (0.9, 0.99)\n",
        "    weight_decay: float = 1e-4\n",
        "    amp: bool = True\n",
        "\n",
        "    # Latent / Model\n",
        "    z_dim: int = 512\n",
        "    expert_hidden: int = 512\n",
        "\n",
        "    # MoE / Routing\n",
        "    init_experts: int = 32\n",
        "    max_experts: int = 256\n",
        "    top_k: int = 8\n",
        "    epsilon_explore: float = 0.1          # 탐험 비율(에폭 지나며 0.01로 감소)\n",
        "    gate_tau_start: float = 2.0\n",
        "    gate_tau_end: float = 0.7\n",
        "\n",
        "    # Plasticity / Stats\n",
        "    ema_decay: float = 0.9\n",
        "    baseline_grad_eps: float = 0.005\n",
        "\n",
        "    # Pruning / Spin-off\n",
        "    prune_usage_thresh: float = 0.01      # 에폭 EMA 기준 사용률\n",
        "    prune_contrib_thresh: float = 1e-4    # 에폭 EMA 기준 기여도\n",
        "    prune_patience_epochs: int = 3\n",
        "    spin_usage_thresh: float = 0.35       # 한 전문가가 과점이면 분할(스핀오프)\n",
        "    spin_interval_epochs: int = 2\n",
        "    spin_noise_std: float = 0.02\n",
        "\n",
        "    # Saving\n",
        "    out_ckpt_dir: str = \"./checkpoints\"\n",
        "    out_samples_dir: str = \"./samples\"\n",
        "    sample_rows: int = 4\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = Config()\n",
        "random.seed(cfg.seed)\n",
        "torch.manual_seed(cfg.seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[Device] {device}\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# Dataset\n",
        "# ---------------------------------------\n",
        "IMG_EXTS = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\")\n",
        "\n",
        "class PlainImageDataset(Dataset):\n",
        "    def __init__(self, root: str, size: int = 128, train: bool = True):\n",
        "        self.paths = [p for p in glob.glob(os.path.join(root, \"**\", \"*\"), recursive=True)\n",
        "                      if os.path.splitext(p)[1].lower() in IMG_EXTS]\n",
        "        if not self.paths:\n",
        "            raise RuntimeError(f\"No images found under {root}. Put 128x128 images in that folder (recursively).\")\n",
        "        aug = [\n",
        "            transforms.Resize(size),\n",
        "            transforms.CenterCrop(size),\n",
        "        ]\n",
        "        if train:\n",
        "            aug.insert(1, transforms.RandomHorizontalFlip(p=0.5))\n",
        "            aug.insert(2, transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05, hue=0.02))\n",
        "        self.tf = transforms.Compose(aug + [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5]*3, [0.5]*3)  # [-1, 1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        return self.tf(img)\n",
        "\n",
        "full_ds = PlainImageDataset(cfg.data_root, size=cfg.image_size, train=True)\n",
        "val_ratio = 0.02 if len(full_ds) > 2000 else 0.1\n",
        "val_len = max(16, int(len(full_ds) * val_ratio))\n",
        "train_len = len(full_ds) - val_len\n",
        "train_ds, val_ds = random_split(full_ds, [train_len, val_len], generator=torch.Generator().manual_seed(cfg.seed))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "                          num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,\n",
        "                        num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
        "print(f\"[Data] train={len(train_ds)} val={len(val_ds)} batch={cfg.batch_size}\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# Model: Conv AE + Latent MoE\n",
        "# ---------------------------------------\n",
        "def conv_block(in_ch, out_ch, k=3, s=1, p=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_ch, out_ch, k, s, p),\n",
        "        nn.GroupNorm(8, out_ch),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, z_dim=512):\n",
        "        super().__init__()\n",
        "        ch = 64\n",
        "        self.net = nn.Sequential(\n",
        "            conv_block(3, ch),\n",
        "            conv_block(ch, ch),\n",
        "            conv_block(ch, ch*2, s=2),          # 64\n",
        "            conv_block(ch*2, ch*2),\n",
        "            conv_block(ch*2, ch*4, s=2),        # 32\n",
        "            conv_block(ch*4, ch*4),\n",
        "            conv_block(ch*4, ch*8, s=2),        # 16\n",
        "            conv_block(ch*8, ch*8),\n",
        "            conv_block(ch*8, ch*8, s=2),        # 8\n",
        "            conv_block(ch*8, ch*8),\n",
        "        )\n",
        "        self.proj = nn.Linear(ch*8*8*8, z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.net(x).view(x.size(0), -1)\n",
        "        z = self.proj(h)\n",
        "        return z\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_dim=512):\n",
        "        super().__init__()\n",
        "        ch = 64\n",
        "        self.fc = nn.Linear(z_dim, ch*8*8*8)  # 8x8x512\n",
        "        self.up = nn.Sequential(\n",
        "            conv_block(ch*8, ch*8),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),   # 16\n",
        "            conv_block(ch*8, ch*8),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),   # 32\n",
        "            conv_block(ch*8, ch*4),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),   # 64\n",
        "            conv_block(ch*4, ch*2),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),   # 128\n",
        "            conv_block(ch*2, ch),\n",
        "            nn.Conv2d(ch, 3, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = self.fc(z).view(z.size(0), 512, 8, 8)\n",
        "        return self.up(h)\n",
        "\n",
        "class ExpertMLP(nn.Module):\n",
        "    def __init__(self, z_dim=512, hidden=512):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(z_dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, z_dim)\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.fc1.bias); nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = F.silu(self.fc1(z))\n",
        "        return self.fc2(h)\n",
        "\n",
        "class MoELatent(nn.Module):\n",
        "    def __init__(self, z_dim=512, hidden=512, init_experts=32):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.hidden = hidden\n",
        "        self.experts = nn.ModuleList([ExpertMLP(z_dim, hidden) for _ in range(init_experts)])\n",
        "        self.prototypes = nn.Parameter(torch.randn(init_experts, z_dim) * 0.02)  # E x D\n",
        "\n",
        "    def num_experts(self): return len(self.experts)\n",
        "\n",
        "    def add_expert(self, clone_idx=None, noise_std=0.02):\n",
        "        new = ExpertMLP(self.z_dim, self.hidden)\n",
        "        if clone_idx is not None and 0 <= clone_idx < len(self.experts):\n",
        "            new.load_state_dict(self.experts[clone_idx].state_dict())\n",
        "            with torch.no_grad():\n",
        "                for p in new.parameters():\n",
        "                    p.add_(torch.randn_like(p) * noise_std)\n",
        "            proto = self.prototypes.data[clone_idx].clone()\n",
        "            proto.add_(torch.randn_like(proto) * noise_std)\n",
        "        else:\n",
        "            proto = torch.randn(self.z_dim, device=self.prototypes.device) * noise_std\n",
        "        self.experts.append(new)\n",
        "        self.prototypes = nn.Parameter(torch.cat([self.prototypes.data, proto[None, :]], dim=0))\n",
        "\n",
        "    def remove_experts(self, indices: List[int]):\n",
        "        if not indices: return\n",
        "        keep = [i for i in range(len(self.experts)) if i not in set(indices)]\n",
        "        self.experts = nn.ModuleList([self.experts[i] for i in keep])\n",
        "        with torch.no_grad():\n",
        "            self.prototypes = nn.Parameter(self.prototypes.data[keep])\n",
        "\n",
        "    def forward(self, z, top_k=8, tau=1.0, epsilon=0.0):\n",
        "        # gating via similarity to prototypes\n",
        "        logits = F.linear(z, self.prototypes) / max(tau, 1e-6)  # [B,E]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        E = self.num_experts()\n",
        "        k = min(top_k, E)\n",
        "        topk_vals, topk_idx = torch.topk(probs, k=k, dim=-1)  # [B,k]\n",
        "\n",
        "        # epsilon exploration: 일부를 랜덤 치환\n",
        "        if epsilon > 0 and E > k:\n",
        "            B = z.size(0)\n",
        "            mask = (torch.rand(B, device=z.device) < epsilon)\n",
        "            if mask.any():\n",
        "                rand_idx = torch.randint(0, E, (mask.sum(),), device=z.device)\n",
        "                topk_idx[mask, -1] = rand_idx\n",
        "                topk_vals[mask, -1] = probs[mask, :].gather(1, rand_idx.view(-1, 1)).squeeze(1)\n",
        "\n",
        "        moe_sum = torch.zeros_like(z)\n",
        "        used_mask = torch.zeros(z.size(0), E, dtype=torch.bool, device=z.device)\n",
        "\n",
        "        # batched expert calls grouped by index\n",
        "        for j in range(k):\n",
        "            idx_j = topk_idx[:, j]\n",
        "            w_j = topk_vals[:, j].unsqueeze(1)\n",
        "            uniq = idx_j.unique()\n",
        "            out_j = torch.zeros_like(z)\n",
        "            for u in uniq:\n",
        "                sel = (idx_j == u)\n",
        "                if sel.any():\n",
        "                    y = self.experts[int(u)](z[sel])\n",
        "                    out_j[sel] = y\n",
        "                    used_mask[sel, int(u)] = True\n",
        "            moe_sum = moe_sum + w_j * out_j\n",
        "\n",
        "        z_out = z + moe_sum\n",
        "        return z_out, probs, used_mask, topk_idx, topk_vals\n",
        "\n",
        "class AutoEncoderMoE(nn.Module):\n",
        "    def __init__(self, z_dim=512, hidden=512, init_experts=32):\n",
        "        super().__init__()\n",
        "        self.enc = Encoder(z_dim=z_dim)\n",
        "        self.moe = MoELatent(z_dim=z_dim, hidden=hidden, init_experts=init_experts)\n",
        "        self.dec = Decoder(z_dim=z_dim)\n",
        "\n",
        "    def forward(self, x, top_k=8, tau=1.0, epsilon=0.0):\n",
        "        z = self.enc(x)\n",
        "        z_moe, probs, used_mask, topk_idx, topk_vals = self.moe(z, top_k=top_k, tau=tau, epsilon=epsilon)\n",
        "        x_rec = self.dec(z_moe)\n",
        "        return x_rec, z, z_moe, probs, used_mask, topk_idx, topk_vals\n",
        "\n",
        "# ---------------------------------------\n",
        "# Utils\n",
        "# ---------------------------------------\n",
        "def denorm(x): return (x.clamp(-1,1) + 1.0) * 0.5\n",
        "\n",
        "def save_sample_grid(epoch, model, loader, device, out_dir, rows=4):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        imgs = next(iter(loader))[:rows*rows].to(device)\n",
        "        rec, *_ = model(imgs, top_k=cfg.top_k, tau=1.0, epsilon=0.0)\n",
        "        grid = make_grid(torch.cat([denorm(imgs), denorm(rec)], dim=0), nrow=rows)\n",
        "        save_image(grid, os.path.join(out_dir, f\"epoch_{epoch:03d}.png\"))\n",
        "    model.train()\n",
        "\n",
        "# ---------------------------------------\n",
        "# Train Setup\n",
        "# ---------------------------------------\n",
        "model = AutoEncoderMoE(cfg.z_dim, cfg.expert_hidden, cfg.init_experts).to(device)\n",
        "\n",
        "if _HAS_LPIPS:\n",
        "    percept = lpips.LPIPS(net='vgg').to(device).eval()\n",
        "else:\n",
        "    percept = None\n",
        "    print(\"[Info] LPIPS 미사용(MSE만 적용). pip 설치가 불가하거나 네트워크가 막힌 환경일 수 있습니다.\")\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, betas=cfg.betas, weight_decay=cfg.weight_decay)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
        "\n",
        "# Expert Stats (에폭 EMA / 사용/기여 추적)\n",
        "class ExpertStats:\n",
        "    def __init__(self, E: int):\n",
        "        self.reset(E)\n",
        "    def reset(self, E):\n",
        "        self.E = E\n",
        "        dev = device\n",
        "        self.usage_epoch  = torch.zeros(E, dtype=torch.float32, device=dev)\n",
        "        self.usage_ema    = torch.zeros(E, dtype=torch.float32, device=dev)\n",
        "        self.contrib_epoch= torch.zeros(E, dtype=torch.float32, device=dev)\n",
        "        self.contrib_ema  = torch.zeros(E, dtype=torch.float32, device=dev)\n",
        "        self.fail_epochs  = torch.zeros(E, dtype=torch.int32 , device=dev)  # pruning patience\n",
        "        self.quarantine   = torch.zeros(E, dtype=torch.int32 , device=dev)  # (미세조정 여지)\n",
        "    def on_structure_change(self, newE, kept=None):\n",
        "        # 구조 변경 시 통계 리맵\n",
        "        ou, oc, of, oq = [x.detach().clone().cpu().tolist() for x in\n",
        "                          [self.usage_ema, self.contrib_ema, self.fail_epochs, self.quarantine]]\n",
        "        dev = device\n",
        "        if kept is None:\n",
        "            self.reset(newE); return\n",
        "        self.E = newE\n",
        "        self.usage_epoch   = torch.zeros(newE, dtype=torch.float32, device=dev)\n",
        "        self.contrib_epoch = torch.zeros(newE, dtype=torch.float32, device=dev)\n",
        "        self.usage_ema     = torch.zeros(newE, dtype=torch.float32, device=dev)\n",
        "        self.contrib_ema   = torch.zeros(newE, dtype=torch.float32, device=dev)\n",
        "        self.fail_epochs   = torch.zeros(newE, dtype=torch.int32 , device=dev)\n",
        "        self.quarantine    = torch.zeros(newE, dtype=torch.int32 , device=dev)\n",
        "        for new_i, old_i in enumerate(kept):\n",
        "            self.usage_ema[new_i]   = torch.tensor(ou[old_i], device=dev)\n",
        "            self.contrib_ema[new_i] = torch.tensor(oc[old_i], device=dev)\n",
        "            self.fail_epochs[new_i] = torch.tensor(of[old_i], device=dev)\n",
        "            self.quarantine[new_i]  = torch.tensor(oq[old_i], device=dev)\n",
        "    def epoch_decay(self, decay):\n",
        "        self.usage_ema   = decay*self.usage_ema   + (1-decay)*self.usage_epoch\n",
        "        self.contrib_ema = decay*self.contrib_ema + (1-decay)*self.contrib_epoch\n",
        "        self.usage_epoch.zero_(); self.contrib_epoch.zero_()\n",
        "\n",
        "stats = ExpertStats(model.moe.num_experts())\n",
        "\n",
        "# ---------------------------------------\n",
        "# Helpers: 구조 조정(프루닝/스핀오프), 요약\n",
        "# ---------------------------------------\n",
        "def maybe_spin_or_prune(epoch, model, stats, opt):\n",
        "    E = model.moe.num_experts()\n",
        "    total_usage = stats.usage_ema.sum().item() + 1e-6\n",
        "    usage_ratio = stats.usage_ema / total_usage\n",
        "\n",
        "    # Prune\n",
        "    prune_idx = []\n",
        "    for i in range(E):\n",
        "        if usage_ratio[i].item() < cfg.prune_usage_thresh and stats.contrib_ema[i].item() < cfg.prune_contrib_thresh:\n",
        "            stats.fail_epochs[i] += 1\n",
        "            if stats.fail_epochs[i] >= cfg.prune_patience_epochs and (E - len(prune_idx) > 4):\n",
        "                prune_idx.append(i)\n",
        "        else:\n",
        "            stats.fail_epochs[i] = 0\n",
        "    note = \"\"\n",
        "    if prune_idx:\n",
        "        keep = [j for j in range(E) if j not in set(int(x) for x in prune_idx)]\n",
        "        model.moe.remove_experts([int(x) for x in prune_idx])\n",
        "        stats.on_structure_change(len(keep), kept=keep)\n",
        "        del opt\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, betas=cfg.betas, weight_decay=cfg.weight_decay)\n",
        "        note += f\"Pruned {len(prune_idx)} | \"\n",
        "\n",
        "    # Spin-off\n",
        "    E = model.moe.num_experts()\n",
        "    if epoch % cfg.spin_interval_epochs == 0 and E < cfg.max_experts and E > 0:\n",
        "        top_usage, top_idx = torch.topk(usage_ratio[:E], k=1)\n",
        "        if top_usage.item() > cfg.spin_usage_thresh:\n",
        "            model.moe.add_expert(clone_idx=int(top_idx.item()), noise_std=cfg.spin_noise_std)\n",
        "            kept = list(range(E)) + [E]\n",
        "            stats.on_structure_change(E+1, kept=kept)\n",
        "            del opt\n",
        "            opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, betas=cfg.betas, weight_decay=cfg.weight_decay)\n",
        "            note += f\"Spun-off from expert {int(top_idx.item())} -> new {E}\"\n",
        "    return opt, note.strip()\n",
        "\n",
        "def epoch_summary(epoch, model, stats, train_loss, val_loss):\n",
        "    E = model.moe.num_experts()\n",
        "    contrib = stats.contrib_ema.detach().cpu()\n",
        "    usage = stats.usage_ema.detach().cpu()\n",
        "    if E > 0:\n",
        "        k = min(5, E)\n",
        "        vals, idx = torch.topk(contrib, k=k)\n",
        "        top5 = [(int(idx[j]), float(vals[j]), float(usage[int(idx[j])])) for j in range(k)]\n",
        "    else:\n",
        "        top5 = []\n",
        "    neuron_scale = E * cfg.expert_hidden\n",
        "\n",
        "    print(f\"\\n[Epoch {epoch}] Experts={E} | NeuronScale={neuron_scale} | TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f}\")\n",
        "    print(\"Top-5 experts (id, contrib_ema, usage_ema):\")\n",
        "    for eid, c, u in top5:\n",
        "        print(f\"  - {eid:3d} | contrib={c:.6f} | usage={u:.6f}\")\n",
        "\n",
        "    summary = {\n",
        "        \"epoch\": epoch,\n",
        "        \"experts_total\": E,\n",
        "        \"neuron_scale\": neuron_scale,\n",
        "        \"top5_by_contrib\": [{\"expert_id\": eid, \"contrib_ema\": c, \"usage_ema\": u} for (eid,c,u) in top5],\n",
        "        \"train_loss\": float(train_loss),\n",
        "        \"val_loss\": float(val_loss),\n",
        "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    }\n",
        "    with open(os.path.join(cfg.out_ckpt_dir, f\"epoch_{epoch:03d}_summary.json\"), \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    return summary\n",
        "\n",
        "# ---------------------------------------\n",
        "# Train Loop\n",
        "# ---------------------------------------\n",
        "global_step = 0\n",
        "for epoch in range(1, cfg.epochs+1):\n",
        "    model.train()\n",
        "    tau = cfg.gate_tau_start + (cfg.gate_tau_end - cfg.gate_tau_start) * (epoch-1)/max(1, cfg.epochs-1)\n",
        "    eps = cfg.epsilon_explore * (1 - (epoch-1)/max(1, cfg.epochs-1)) + 0.01\n",
        "\n",
        "    running = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg.epochs} (train)\")\n",
        "    for batch in pbar:\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "\n",
        "        # baseline 경로(전문가 없이)로 재구성 손실 측정\n",
        "        with torch.no_grad():\n",
        "            z_base = model.enc(batch)\n",
        "            x_base = model.dec(z_base)\n",
        "            base_mse = F.mse_loss(x_base, batch)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
        "            x_rec, z0, z_moe, probs, used_mask, topk_idx, topk_vals = model(batch, top_k=cfg.top_k, tau=tau, epsilon=eps)\n",
        "            mse = F.mse_loss(x_rec, batch)\n",
        "            if _HAS_LPIPS:\n",
        "                lp = percept(denorm(x_rec), denorm(batch)).mean()\n",
        "                loss = 0.7*mse + 0.3*lp\n",
        "            else:\n",
        "                loss = mse\n",
        "                lp = torch.tensor(0.0, device=device)\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "\n",
        "        # 사용/기여 통계 (에폭 누적)\n",
        "        with torch.no_grad():\n",
        "            B = batch.size(0)\n",
        "            E = model.moe.num_experts()\n",
        "            if E > 0:\n",
        "                stats.usage_epoch[:E] += used_mask.float().sum(dim=0)\n",
        "                improv = (base_mse - loss).clamp_min(0.0)  # 전체 개선 추정치\n",
        "                per_exp_weight = torch.zeros(E, device=device)\n",
        "                k = topk_idx.size(1)\n",
        "                for j in range(k):\n",
        "                    idx_j = topk_idx[:, j]\n",
        "                    w_j = topk_vals[:, j]\n",
        "                    for u in idx_j.unique():\n",
        "                        sel = (idx_j == u)\n",
        "                        per_exp_weight[int(u)] += w_j[sel].sum()\n",
        "                if per_exp_weight.sum() > 0:\n",
        "                    per_exp_weight = per_exp_weight / per_exp_weight.sum()\n",
        "                    stats.contrib_epoch[:E] += per_exp_weight * improv.item()\n",
        "\n",
        "        running += loss.item()\n",
        "        global_step += 1\n",
        "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", mse=f\"{mse.item():.4f}\", lpips=f\"{lp.item():.4f}\", tau=f\"{tau:.2f}\", eps=f\"{eps:.3f}\")\n",
        "\n",
        "    # EMA 업데이트\n",
        "    stats.epoch_decay(cfg.ema_decay)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch}/{cfg.epochs} (val)\"):\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
        "                x_rec, *_ = model(batch, top_k=cfg.top_k, tau=tau, epsilon=0.0)\n",
        "                mse = F.mse_loss(x_rec, batch)\n",
        "                if _HAS_LPIPS:\n",
        "                    lp = percept(denorm(x_rec), denorm(batch)).mean()\n",
        "                    loss = 0.7*mse + 0.3*lp\n",
        "                else:\n",
        "                    loss = mse\n",
        "            val_loss += loss.item() * batch.size(0)\n",
        "    val_loss /= len(val_ds)\n",
        "\n",
        "    # 구조 조정 (Prune / Spin-off)\n",
        "    opt, note = maybe_spin_or_prune(epoch, model, stats, opt)\n",
        "    if note:\n",
        "        print(f\"[Structure] {note} | Experts={model.moe.num_experts()}\")\n",
        "\n",
        "    # 샘플/체크포인트 저장\n",
        "    save_sample_grid(epoch, model, val_loader, device, cfg.out_samples_dir, rows=cfg.sample_rows)\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": opt.state_dict(),\n",
        "        \"stats\": {\n",
        "            \"usage_ema\": stats.usage_ema.detach().cpu().tolist(),\n",
        "            \"contrib_ema\": stats.contrib_ema.detach().cpu().tolist(),\n",
        "            \"fail_epochs\": stats.fail_epochs.detach().cpu().tolist(),\n",
        "            \"quarantine\": stats.quarantine.detach().cpu().tolist(),\n",
        "        },\n",
        "        \"config\": cfg.__dict__,\n",
        "    }\n",
        "    torch.save(ckpt, os.path.join(cfg.out_ckpt_dir, f\"epoch_{epoch:03d}.pt\"))\n",
        "\n",
        "    # 에폭 요약 출력/저장\n",
        "    _ = epoch_summary(epoch, model, stats, train_loss=running/len(train_loader), val_loss=val_loss)\n",
        "\n",
        "print(\"✅ Training complete. Check ./checkpoints and ./samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IjdzPrAeTEWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}